{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "#Fundamental librarys to math and stats process\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import scipy.stats as ss\n",
    "import math\n",
    "#data prepared\n",
    "import pandas as pd\n",
    "\n",
    "#ML preprocessi\n",
    "from sklearn import preprocessing\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import feature_selection as fs\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler,Normalizer, MinMaxScaler,FunctionTransformer, PolynomialFeatures\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "\n",
    "# ML algorithms models\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "\n",
    "# ML Evaluations\n",
    "import sklearn.metrics as sklm\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import fancyimpute\n",
    "#from ._conv import register_converters as _register_converters\n",
    "from imblearn.pipeline import make_pipeline as imb_make_pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.ensemble import BalancedBaggingClassifier, EasyEnsemble\n",
    "from mlens.visualization import corrmat\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.preprocessing import Imputer, RobustScaler, FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (roc_auc_score, confusion_matrix,classification_report,\n",
    "                             accuracy_score, roc_curve,\n",
    "                             precision_recall_curve, f1_score)\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import xgboost as xgb\n",
    "from keras import models, layers, optimizers\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_enc = (pd.read_csv('../Data/df_enc_3.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>property_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>occupancy</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>preapproval</th>\n",
       "      <th>msa_md</th>\n",
       "      <th>state_code</th>\n",
       "      <th>county_code</th>\n",
       "      <th>...</th>\n",
       "      <th>applicant_race_Asian</th>\n",
       "      <th>applicant_race_African_American</th>\n",
       "      <th>applicant_race_Native_Hawaiian</th>\n",
       "      <th>applicant_race_White</th>\n",
       "      <th>applicant_race_Information_not_provided</th>\n",
       "      <th>applicant_race_Not_applicable</th>\n",
       "      <th>applicant_sex_Male</th>\n",
       "      <th>applicant_sex_Female</th>\n",
       "      <th>applicant_sex_Information_not_provided</th>\n",
       "      <th>applicant_sex_Not_applicable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>37</td>\n",
       "      <td>246</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>3</td>\n",
       "      <td>369</td>\n",
       "      <td>52</td>\n",
       "      <td>299</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>306</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>305</td>\n",
       "      <td>47</td>\n",
       "      <td>180</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>305</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>37</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  loan_type  property_type  loan_purpose  occupancy  loan_amount  \\\n",
       "0           0          3              1             1          1           70   \n",
       "1           1          1              1             3          1          178   \n",
       "2           2          2              1             3          1          163   \n",
       "3           3          1              1             1          1          155   \n",
       "4           4          1              1             1          1          305   \n",
       "\n",
       "   preapproval  msa_md  state_code  county_code              ...               \\\n",
       "0            3      18          37          246              ...                \n",
       "1            3     369          52          299              ...                \n",
       "2            3      16          10          306              ...                \n",
       "3            1     305          47          180              ...                \n",
       "4            3      24          37           20              ...                \n",
       "\n",
       "   applicant_race_Asian  applicant_race_African_American  \\\n",
       "0                     0                                0   \n",
       "1                     0                                0   \n",
       "2                     0                                0   \n",
       "3                     0                                0   \n",
       "4                     0                                1   \n",
       "\n",
       "   applicant_race_Native_Hawaiian  applicant_race_White  \\\n",
       "0                               0                     1   \n",
       "1                               0                     1   \n",
       "2                               0                     1   \n",
       "3                               0                     1   \n",
       "4                               0                     0   \n",
       "\n",
       "   applicant_race_Information_not_provided  applicant_race_Not_applicable  \\\n",
       "0                                        0                              0   \n",
       "1                                        0                              0   \n",
       "2                                        0                              0   \n",
       "3                                        0                              0   \n",
       "4                                        0                              0   \n",
       "\n",
       "   applicant_sex_Male  applicant_sex_Female  \\\n",
       "0                   1                     0   \n",
       "1                   1                     0   \n",
       "2                   1                     0   \n",
       "3                   1                     0   \n",
       "4                   0                     1   \n",
       "\n",
       "   applicant_sex_Information_not_provided  applicant_sex_Not_applicable  \n",
       "0                                       0                             0  \n",
       "1                                       0                             0  \n",
       "2                                       0                             0  \n",
       "3                                       0                             0  \n",
       "4                                       0                             0  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x= df.drop(['Unnamed: 0', 'accepted'], axis=1)\n",
    "#y= df['accepted']\n",
    "\n",
    "df= df.drop(['Unnamed: 0'], axis=1)\n",
    "#y= df['accepted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 54)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_type                                       0\n",
       "property_type                                   0\n",
       "loan_purpose                                    0\n",
       "occupancy                                       0\n",
       "loan_amount                                     0\n",
       "preapproval                                     0\n",
       "msa_md                                          0\n",
       "state_code                                      0\n",
       "county_code                                     0\n",
       "applicant_ethnicity                             0\n",
       "applicant_race                                  0\n",
       "applicant_sex                                   0\n",
       "applicant_income                                0\n",
       "population                                      0\n",
       "minority_population_pct                         0\n",
       "ffiecmedian_family_income                       0\n",
       "tract_to_msa_md_income_pct                      0\n",
       "number_of_owner-occupied_units                  0\n",
       "number_of_1_to_4_family_units                   0\n",
       "lender                                          0\n",
       "accepted                                        0\n",
       "co_applicant_True                               0\n",
       "co_applicant_False                              0\n",
       "loan_type_conv                                  0\n",
       "loan_type_FHA                                   0\n",
       "loan_type_VA                                    0\n",
       "loan_type_FSA_RHS                               0\n",
       "property_type_One_to_four_family                0\n",
       "property_type_Manufactured_housing              0\n",
       "property_type_Multifamily                       0\n",
       "loan_purpose_Home_purchase                      0\n",
       "loan_purpose_Home_improvement                   0\n",
       "loan_purpose_Refinancing                        0\n",
       "occupancy_Owner_occupied                        0\n",
       "occupancy_Not_owner_occupied                    0\n",
       "occupancy_Not_applicable                        0\n",
       "preapproval_Preapproval_requested               0\n",
       "preapproval_Preapproval_not_requested           0\n",
       "preapproval_Not_applicable                      0\n",
       "applicant_ethnicity_Hispanic_Latino             0\n",
       "applicant_ethnicity_Not_Hispanic_Latino         0\n",
       "applicant_ethnicity_Information_not_provided    0\n",
       "applicant_ethnicity_Not_applicable              0\n",
       "applicant_race_American_Indian                  0\n",
       "applicant_race_Asian                            0\n",
       "applicant_race_African_American                 0\n",
       "applicant_race_Native_Hawaiian                  0\n",
       "applicant_race_White                            0\n",
       "applicant_race_Information_not_provided         0\n",
       "applicant_race_Not_applicable                   0\n",
       "applicant_sex_Male                              0\n",
       "applicant_sex_Female                            0\n",
       "applicant_sex_Information_not_provided          0\n",
       "applicant_sex_Not_applicable                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_type</th>\n",
       "      <th>property_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>occupancy</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>preapproval</th>\n",
       "      <th>msa_md</th>\n",
       "      <th>state_code</th>\n",
       "      <th>county_code</th>\n",
       "      <th>applicant_ethnicity</th>\n",
       "      <th>...</th>\n",
       "      <th>applicant_race_Asian</th>\n",
       "      <th>applicant_race_African_American</th>\n",
       "      <th>applicant_race_Native_Hawaiian</th>\n",
       "      <th>applicant_race_White</th>\n",
       "      <th>applicant_race_Information_not_provided</th>\n",
       "      <th>applicant_race_Not_applicable</th>\n",
       "      <th>applicant_sex_Male</th>\n",
       "      <th>applicant_sex_Female</th>\n",
       "      <th>applicant_sex_Information_not_provided</th>\n",
       "      <th>applicant_sex_Not_applicable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>37</td>\n",
       "      <td>246</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>3</td>\n",
       "      <td>369</td>\n",
       "      <td>52</td>\n",
       "      <td>299</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>306</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>305</td>\n",
       "      <td>47</td>\n",
       "      <td>180</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>305</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>37</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_type  property_type  loan_purpose  occupancy  loan_amount  \\\n",
       "0          3              1             1          1           70   \n",
       "1          1              1             3          1          178   \n",
       "2          2              1             3          1          163   \n",
       "3          1              1             1          1          155   \n",
       "4          1              1             1          1          305   \n",
       "\n",
       "   preapproval  msa_md  state_code  county_code  applicant_ethnicity  \\\n",
       "0            3      18          37          246                    2   \n",
       "1            3     369          52          299                    1   \n",
       "2            3      16          10          306                    2   \n",
       "3            1     305          47          180                    2   \n",
       "4            3      24          37           20                    2   \n",
       "\n",
       "               ...               applicant_race_Asian  \\\n",
       "0              ...                                  0   \n",
       "1              ...                                  0   \n",
       "2              ...                                  0   \n",
       "3              ...                                  0   \n",
       "4              ...                                  0   \n",
       "\n",
       "   applicant_race_African_American  applicant_race_Native_Hawaiian  \\\n",
       "0                                0                               0   \n",
       "1                                0                               0   \n",
       "2                                0                               0   \n",
       "3                                0                               0   \n",
       "4                                1                               0   \n",
       "\n",
       "   applicant_race_White  applicant_race_Information_not_provided  \\\n",
       "0                     1                                        0   \n",
       "1                     1                                        0   \n",
       "2                     1                                        0   \n",
       "3                     1                                        0   \n",
       "4                     0                                        0   \n",
       "\n",
       "   applicant_race_Not_applicable  applicant_sex_Male  applicant_sex_Female  \\\n",
       "0                              0                   1                     0   \n",
       "1                              0                   1                     0   \n",
       "2                              0                   1                     0   \n",
       "3                              0                   1                     0   \n",
       "4                              0                   0                     1   \n",
       "\n",
       "   applicant_sex_Information_not_provided  applicant_sex_Not_applicable  \n",
       "0                                       0                             0  \n",
       "1                                       0                             0  \n",
       "2                                       0                             0  \n",
       "3                                       0                             0  \n",
       "4                                       0                             0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create binary features to check if the example is has missing values for all features that have missing values\n",
    "for feature in df.columns:\n",
    "    if np.any(np.isnan(df[feature])):\n",
    "        df[\"is_\" + feature + \"_missing\"] = np.isnan(df[feature]) * 1\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 53)\n",
      "(500000,)\n"
     ]
    }
   ],
   "source": [
    "x=df.drop('accepted', axis = 1)\n",
    "# Convert to numpy array\n",
    "x=np.array(x)\n",
    "# Labels are the values we want to predict\n",
    "y= df['accepted']\n",
    "# Use numpy to convert to arrays\n",
    "y=np.array(y)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loan_type',\n",
       " 'property_type',\n",
       " 'loan_purpose',\n",
       " 'occupancy',\n",
       " 'loan_amount',\n",
       " 'preapproval',\n",
       " 'msa_md',\n",
       " 'state_code',\n",
       " 'county_code',\n",
       " 'applicant_ethnicity',\n",
       " 'applicant_race',\n",
       " 'applicant_sex',\n",
       " 'applicant_income',\n",
       " 'population',\n",
       " 'minority_population_pct',\n",
       " 'ffiecmedian_family_income',\n",
       " 'tract_to_msa_md_income_pct',\n",
       " 'number_of_owner-occupied_units',\n",
       " 'number_of_1_to_4_family_units',\n",
       " 'lender',\n",
       " 'accepted',\n",
       " 'co_applicant_True',\n",
       " 'co_applicant_False',\n",
       " 'loan_type_conv',\n",
       " 'loan_type_FHA',\n",
       " 'loan_type_VA',\n",
       " 'loan_type_FSA_RHS',\n",
       " 'property_type_One_to_four_family',\n",
       " 'property_type_Manufactured_housing',\n",
       " 'property_type_Multifamily',\n",
       " 'loan_purpose_Home_purchase',\n",
       " 'loan_purpose_Home_improvement',\n",
       " 'loan_purpose_Refinancing',\n",
       " 'occupancy_Owner_occupied',\n",
       " 'occupancy_Not_owner_occupied',\n",
       " 'occupancy_Not_applicable',\n",
       " 'preapproval_Preapproval_requested',\n",
       " 'preapproval_Preapproval_not_requested',\n",
       " 'preapproval_Not_applicable',\n",
       " 'applicant_ethnicity_Hispanic_Latino',\n",
       " 'applicant_ethnicity_Not_Hispanic_Latino',\n",
       " 'applicant_ethnicity_Information_not_provided',\n",
       " 'applicant_ethnicity_Not_applicable',\n",
       " 'applicant_race_American_Indian',\n",
       " 'applicant_race_Asian',\n",
       " 'applicant_race_African_American',\n",
       " 'applicant_race_Native_Hawaiian',\n",
       " 'applicant_race_White',\n",
       " 'applicant_race_Information_not_provided',\n",
       " 'applicant_race_Not_applicable',\n",
       " 'applicant_sex_Male',\n",
       " 'applicant_sex_Female',\n",
       " 'applicant_sex_Information_not_provided',\n",
       " 'applicant_sex_Not_applicable']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving feature names for later use\n",
    "#df = df.drop('accepted', axis = 1)\n",
    "feature_list = list(df.columns)\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select features according to the k highest scores.\n",
    "some sort of normalized values such as z-scores and therefore don't want to do any more normalization,then you should consider using the ANOVA (f_classif) scoring function for your feature selection. If you are using z-score normalization or some other normalization that uses negatives (maybe your data falls between -1 and +1),you could just use f_classif scoring function which doesn't require only positive numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Specs         Score\n",
      "29          property_type_Multifamily  13106.289826\n",
      "2                        loan_purpose   8811.125900\n",
      "36  preapproval_Preapproval_requested   6742.605950\n",
      "30         loan_purpose_Home_purchase   6301.742257\n",
      "27   property_type_One_to_four_family   6265.408954\n",
      "44               applicant_race_Asian   5445.969410\n",
      "46     applicant_race_Native_Hawaiian   5299.462495\n",
      "21                  co_applicant_True   5165.009020\n",
      "20                           accepted   5165.009020\n",
      "35           occupancy_Not_applicable   5118.845798\n",
      "26                  loan_type_FSA_RHS   4923.917086\n",
      "31      loan_purpose_Home_improvement   4263.821084\n",
      "7                          state_code   4132.043896\n",
      "1                       property_type   3269.627173\n"
     ]
    }
   ],
   "source": [
    "bestfeatures = fs.SelectKBest(score_func=fs.f_classif, k=10)\n",
    "fit = bestfeatures.fit(x,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(df.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(14,'Score'))  #print 10 best features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Train models selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "\n",
    "Random state (Pseudo-random number) in Scikit learn train_test_split splits arrays or matrices into random train and test subsets. That means that everytime you run it without specifying random_state, you will get a different result, this is expected behavior.It changes. On the other hand if you use random_state=some_number, then you can guarantee that the output of Run 1 will be equal to the output of Run 2, i.e. your split will be always the same. It doesn't matter what the actual random_state number is 42, 0, 21, ... The important thing is that everytime you use 42, you will always get the same output the first time you make the split. This is useful if you want reproducible results, for example in the documentation, so that everybody can consistently see the same numbers when they run the examples. In practice I would say, you should set the random_state to some fixed number while you test stuff, but then remove it in production if you really need a random (and not a fixed) split.Regarding your second question, a pseudo-random number generator is a number generator that generates almost truly random numbers. Why they are not truly random is out of the scope of this question and probably won't matter in your case, you can take a look here form more details.Pseudorandom number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shapes: ((460000, 53), (40000, 53))\n",
      "Original data shapes: ((460000,), (40000,))\n"
     ]
    }
   ],
   "source": [
    "# Original Data\n",
    "\n",
    "#x = df.loc[:, df.columns != \"accepted\"].values\n",
    "#y = df.loc[:, df.columns == \"accepted\"].values.flatten()\n",
    "\n",
    "# splt train test\n",
    "nr.seed(9988)\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x,y)\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=54321)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.08, shuffle=True, stratify=y, random_state=54321)\n",
    "print(f\"Original data shapes: {x_train.shape, x_test.shape}\")\n",
    "print(f\"Original data shapes: {y_train.shape, y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [768, 500000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-769a1f5fcdc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-769a1f5fcdc5>\u001b[0m in \u001b[0;36mplot_learning_curve\u001b[0;34m(estimator, title, X, y, ylim, cv, n_jobs, train_sizes)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     train_sizes, train_scores, test_scores = learning_curve(\n\u001b[0;32m---> 74\u001b[0;31m         estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mtrain_scores_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mtrain_scores_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mlearning_curve\u001b[0;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state)\u001b[0m\n\u001b[1;32m   1087\u001b[0m         raise ValueError(\"An estimator must support the partial_fit interface \"\n\u001b[1;32m   1088\u001b[0m                          \"to exploit incremental learning\")\n\u001b[0;32m-> 1089\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m     \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [768, 500000]"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCklSJ4NCktTJoJAkdTIoJEmd/j+KaIKaZIqqYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe91b0e3358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "title = \"Learning Curves (Naive Bayes)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.08, random_state=0)\n",
    "\n",
    "estimator = GaussianNB()\n",
    "plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "title = r\"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.08, random_state=0)\n",
    "estimator = SVC(gamma=0.001)\n",
    "plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=500,\n",
    "                                max_features=0.25,\n",
    "                                criterion=\"entropy\",\n",
    "                                class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit RF to plot feature importances\n",
    "rf.fit(RobustScaler().fit_transform(Imputer(strategy=\"median\").fit_transform(x_train)), y_train)\n",
    "\n",
    "# Plot features importance\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(rf.feature_importances_)[::-1]\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(1, 46), importances[indices], align=\"center\")\n",
    "plt.xticks(range(1, 46), df.columns[df.columns != \"accepted\"][indices], rotation=90)\n",
    "plt.title(\"Feature Importance\", {\"fontsize\": 16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Create Random Forest Regressor object\n",
    "#rf = RandomForestClassifier(max_depth=8, random_state=1)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100,\n",
    "                                max_features=0.25,\n",
    "                                criterion=\"entropy\",\n",
    "                                class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model using the training sets\n",
    "rf.fit(x_train, y_train)\n",
    "# Make predictions using the testing set\n",
    "rf_pred = rf.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Score the model\n",
    "rf_score = f1_score(y_test, rf_pred, average='micro')\n",
    "rf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "confusion_rf = confusion_matrix(y_test, rf_pred)\n",
    "confusion_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['1','2','3']\n",
    "\n",
    "plt.imshow(confusion_rf, cmap=plt.cm.Blues, interpolation='nearest')\n",
    "plt.xticks([0,1,2], columns, rotation='vertical')\n",
    "plt.yticks([0,1,2], columns)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Classification models:\")\n",
    "print(\"Random forest score: \", rf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross validation\n",
    "Results_Forest = cross_validate(rf,x,y,scoring=\"r2\",cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Forest_test_scores = Results_Forest['test_score']\n",
    "Forest_train_scores = Results_Forest['train_score']\n",
    "print('Forest')\n",
    "print(np.mean(Forest_train_scores))\n",
    "print(np.mean(Forest_test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The baseline predictions are the historical averages\n",
    "baseline_preds = x_test[:, feature_list.index('accepted')]\n",
    "# Baseline errors, and display average baseline error\n",
    "baseline_errors = abs(baseline_preds - y_test)\n",
    "print('Average baseline error: ', round(np.mean(baseline_errors), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/confusion-matrix-machine-learning/\n",
    "# Python script for confusion matrix creation. \n",
    "#!/usr/bin/python\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "rng = np.random.RandomState(31337)\n",
    "\n",
    "print(\"Random forest binary classification\")\n",
    "# Build random forest classifier (same config)\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)\n",
    "for train_index, test_index in kf.split(x):\n",
    "    rf_clf = RandomForestClassifier().fit(x[train_index], y[train_index])\n",
    "    # make predictions for test data\n",
    "    predictions = rf_clf.predict(x[test_index])\n",
    "    actuals = y[test_index]\n",
    "    predictions = [round(value) for value in predictions]\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    print('Confusion Matrix :')\n",
    "    print(confusion_matrix(actuals, predictions))\n",
    "    print('Report : ')\n",
    "    print(classification_report(actuals, predictions))\n",
    "\n",
    "\n",
    "nr.seed(3456)\n",
    "## Define the dictionary for the grid search and the model object to search on\n",
    "param_grid_Forest = {\"n_estimators\": [5,3,8],\n",
    "                     'max_features':[2,4,6],\n",
    "                    'max_depth': [2,4,6]}\n",
    "\n",
    "## Perform the grid search over the parameters\n",
    "\n",
    "Grid_Forest = ms.GridSearchCV(estimator = rf_clf, param_grid = param_grid_Forest, verbose=1)\n",
    "\n",
    "                   \n",
    "#'max_depth': [2,4,6],'n_estimators': [5,3,8],'learning_rate': [0.1,0.01],                   \n",
    "Grid_Forest.fit(x,y)\n",
    "print(Grid_Forest.best_score_)\n",
    "print(Grid_Forest.best_params_)\n",
    "    \n",
    "print(\"start of: regression\")\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)\n",
    "for train_index, test_index in kf.split(x):\n",
    "    xgb_model = xgb.XGBRegressor().fit(x[train_index], y[train_index])\n",
    "    predictions = xgb_model.predict(x[test_index])\n",
    "    actuals = y[test_index]\n",
    "    predictions = [round(value) for value in predictions]\n",
    "    # evaluate predictions\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    print('Confusion Matrix :')\n",
    "    print(confusion_matrix(actuals, predictions))\n",
    "    print('Report : ')\n",
    "    print(classification_report(actuals, predictions))\n",
    "    print(mean_squared_error(actuals, predictions))\n",
    "\n",
    "print(\"Parameter optimization\")\n",
    "\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "crf = GridSearchCV(xgb_model,\n",
    "                   {'max_depth': [2,4,6],'n_estimators': [10,20,100],'learning_rate': [0.1,0.15,0.2]}, verbose=1)\n",
    "crf.fit(x,y)\n",
    "print(crf.best_score_)\n",
    "print(crf.best_params_)\n",
    "\n",
    "print(\"End of regressor\")\n",
    "\n",
    "# The sklearn API models are picklable\n",
    "print(\"Pickling sklearn API models\")\n",
    "# must open in binary format to pickle\n",
    "pickle.dump(crf, open(\"Data/best_predict.pkl\", \"wb\"))\n",
    "clf2 = pickle.load(open(\"Data/best_predict.pkl\", \"rb\"))\n",
    "print(np.allclose(crf.predict(x), clf2.predict(x)))\n",
    "# Early-stopping\n",
    "\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05, random_state=54321)\n",
    "clf = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                            learning_rate=0.01,\n",
    "                            n_estimators=100,\n",
    "                            max_depth=6,\n",
    "                            subsample=0.8,\n",
    "                            random_state=54321)\n",
    "\n",
    "eval_set = [(x_test, y_test)]\n",
    "# fit model on training data\n",
    "clf.fit(x_train, y_train, early_stopping_rounds=20, eval_metric=\"auc\",eval_set=eval_set, verbose=True)\n",
    "# make predictions for test data\n",
    "y_pred = clf.predict(x_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-family: Georgia; font-size:1.5em;color:purple; font-style:bold\">\n",
    "Strategies to deal with imbalanced data</h3><br>\n",
    "\n",
    "Classification problems in most real world applications have imbalanced data sets. In other words, the positive examples (minority class) are a lot less than negative examples (majority class). We can see that in spam detection, ads click, loan approvals, etc. In our example, the positive examples (people who haven't fully paid) were only 19% from the total examples. Therefore, accuracy is no longer a good measure of performance for different models because if we simply predict all examples to belong to the negative class, we achieve 81% accuracy. Better metrics for imbalanced data sets are *AUC* (area under the ROC curve) and f1-score. However, that's not enough because class imbalance influences a learning algorithm during training by making the decision rule biased towards the majority class by implicitly learns a model that optimizes the predictions based on the majority class in the dataset. As a result, we'll explore different methods to overcome class imbalance problem.\n",
    "- Under-Sample: Under-sample the majority class with or w/o replacement by making the number of positive and negative examples equal. One of the drawbacks of under-sampling is that it ignores a good portion of training data that has valuable information. In our example, it would loose around 6500 examples. However, it's very fast to train.\n",
    "- Over-Sample: Over-sample the minority class with or w/o replacement by making the number of positive and negative examples equal. We'll add around 6500 samples from the training data set with this strategy. It's a lot more computationally expensive than under-sampling. Also, it's more prune to overfitting due to repeated examples.\n",
    "- EasyEnsemble: Sample several subsets from the majority class, build a classifier on top of each sampled data, and combine the output of all classifiers. More details can be found [here](http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/tsmcb09.pdf).\n",
    "- Synthetic Minority Oversampling Technique (SMOTE): It over-samples the minority class but using synthesized examples. It operates on feature space not the data space. Here how it works:\n",
    "    - Compute the k-nearest neighbors for all minority samples.\n",
    "    - Randomly choose number between 1-k.\n",
    "    - For each feature:\n",
    "        - Compute the difference between minority sample and its randomly chosen neighbor (from previous step).\n",
    "        - Multiply the difference by random number between 0 and 1.\n",
    "        - Add the obtained feature to the synthesized sample attributes.\n",
    "    - Repeat the above until we get the number of synthesized samples needed. More information can be found [here](https://www.jair.org/media/953/live-953-2037-jair.pdf).\n",
    "\n",
    "There are other methods such as `EditedNearestNeighbors` and `CondensedNearestNeighbors` that we will not cover in this notebook and are rarely used in practice.\n",
    "\n",
    "In most applications, misclassifying the minority class (false negative) is a lot more expensive than misclassifying the majority class (false positive). In the context of lending, loosing money by lending to a risky borrower who is more likely to not fully pay the loan back is a lot more costly than missing the opportunity of lending to trust-worthy borrower (less risky). As a result, we can use `class_weight` that changes the weight of misclassifying positive example in the loss function. Also, we can use different cut-offs assign examples to classes. By default, 0.5 is the cut-off; however, we see more often in applications such as lending that the cut-off is less than 0.5. Note that changing the cut-off from the default 0.5 reduce the overall accuracy but may improve the accuracy of predicting positive/negative examples.\n",
    "\n",
    "We'll evaluate all the above methods plus the original model without resampling as a baseline model using the same *Random Forest* classifier we used in the missing values section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build random forest classifier (same config)\n",
    "rf_clf = RandomForestClassifier(n_estimators=500,\n",
    "                                max_features=0.25,\n",
    "                                criterion=\"entropy\",\n",
    "                                class_weight=\"balanced\")\n",
    "\n",
    "# Build model with no sampling\n",
    "pip_orig = make_pipeline(Imputer(strategy=\"mean\"),\n",
    "                         RobustScaler(),\n",
    "                         rf_clf)\n",
    "scores = cross_val_score(pip_orig,\n",
    "                         X_train, y_train,\n",
    "                         scoring=\"roc_auc\", cv=10)\n",
    "print(f\"\\033[1m\\033[94mOriginal model's average AUC: {scores.mean():.3f}\")\n",
    "\n",
    "# Build model with undersampling\n",
    "pip_undersample = imb_make_pipeline(Imputer(strategy=\"mean\"),\n",
    "                                    RobustScaler(),\n",
    "                                    RandomUnderSampler(), rf_clf)\n",
    "scores = cross_val_score(pip_undersample,\n",
    "                         X_train, y_train,\n",
    "                         scoring=\"roc_auc\", cv=10)\n",
    "print(f\"\\033[1m\\033[94mUnder-sampled model's average AUC: {scores.mean():.3f}\")\n",
    "\n",
    "# Build model with oversampling\n",
    "pip_oversample = imb_make_pipeline(Imputer(strategy=\"mean\"),\n",
    "                                    RobustScaler(),\n",
    "                                    RandomOverSampler(), rf_clf)\n",
    "scores = cross_val_score(pip_oversample,\n",
    "                         X_train, y_train,\n",
    "                         scoring=\"roc_auc\", cv=10)\n",
    "print(f\"\\033[1m\\033[94mOver-sampled model's average AUC: {scores.mean():.3f}\")\n",
    "\n",
    "# Build model with EasyEnsemble\n",
    "resampled_rf = BalancedBaggingClassifier(base_estimator=rf_clf,\n",
    "                                         n_estimators=10, random_state=123)\n",
    "pip_resampled = make_pipeline(Imputer(strategy=\"mean\"),\n",
    "                              RobustScaler(), resampled_rf)\n",
    "                             \n",
    "scores = cross_val_score(pip_resampled,\n",
    "                         X_train, y_train,\n",
    "                         scoring=\"roc_auc\", cv=10)\n",
    "print(f\"\\033[1m\\033[94mEasyEnsemble model's average AUC: {scores.mean():.3f}\")\n",
    "\n",
    "# Build model with SMOTE\n",
    "pip_smote = imb_make_pipeline(Imputer(strategy=\"mean\"),\n",
    "                              RobustScaler(),\n",
    "                              SMOTE(), rf_clf)\n",
    "scores = cross_val_score(pip_smote,\n",
    "                         X_train, y_train,\n",
    "                         scoring=\"roc_auc\", cv=10)\n",
    "print(f\"\\033[1m\\033[94mSMOTE model's average AUC: {scores.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Impute the missing data using features means\n",
    "imp = Imputer()\n",
    "imp.fit(x_train)\n",
    "x_train = imp.transform(x_train)\n",
    "x_test = imp.transform(x_test)\n",
    "\n",
    "# Standardize the data\n",
    "std = RobustScaler()\n",
    "std.fit(X_train)\n",
    "x_train = std.transform(x_train)\n",
    "x_test = std.transform(x_test)\n",
    "\n",
    "# Implement RandomUnderSampler\n",
    "random_undersampler = RandomUnderSampler()\n",
    "x_res, y_res = random_undersampler.fit_sample(x_train, y_train)\n",
    "# Shuffle the data\n",
    "perms = np.random.permutation(x_res.shape[0])\n",
    "x_res = x_res[perms]\n",
    "y_res = y_res[perms]\n",
    "x_res.shape, y_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nr.seed(123)\n",
    "inside = ms.KFold(n_splits=5, shuffle = True)\n",
    "nr.seed(321)\n",
    "outside = ms.KFold(n_splits=5, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/confusion-matrix-machine-learning/\n",
    "# Python script for confusion matrix creation. \n",
    "#!/usr/bin/python\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "rng = np.random.RandomState(31337)\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "nr.seed(3456)\n",
    "## Define the dictionary for the grid search and the model object to search on\n",
    "param_grid_Forest = {\"n_estimators\": [50,100],\n",
    "                     'max_features':[0.1,0.2,0.5,0.8],\n",
    "                    'max_depth': [4,6]}\n",
    "\n",
    "## Perform the grid search over the parameters\n",
    "\n",
    "Grid_Forest = ms.GridSearchCV(estimator = rf_model, param_grid = param_grid_Forest, \n",
    "                      cv = 10, # Use the inside folds\n",
    "                      scoring = 'r2',\n",
    "                      return_train_score = True,\n",
    "                      verbose=1)\n",
    "\n",
    "                   \n",
    "#'max_depth': [2,4,6],'n_estimators': [5,3,8],'learning_rate': [0.1,0.01],                   \n",
    "Grid_Forest.fit(x,y)\n",
    "print(Grid_Forest.best_score_)\n",
    "print(Grid_Forest.best_params_)\n",
    "\n",
    "print(\"Random forest binary classification\")\n",
    "# Build random forest classifier (same config)\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)\n",
    "for train_index, test_index in kf.split(x):\n",
    "    rf_clf = RandomForestClassifier(n_estimators=100,\n",
    "                                criterion=\"entropy\",max_features=0.25,max_depth=6,\n",
    "                                class_weight=\"balanced\").fit(x[train_index], y[train_index])\n",
    "    # make predictions for test data\n",
    "    predictions = rf_clf.predict(x[test_index])\n",
    "    actuals = y[test_index]\n",
    "    predictions = [round(value) for value in predictions]\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    print('Confusion Matrix :')\n",
    "    print(confusion_matrix(actuals, predictions))\n",
    "    print('Report : ')\n",
    "    print(classification_report(actuals, predictions))\n",
    "\n",
    "\n",
    "print(\"XGB binary classification\")\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)\n",
    "for train_index, test_index in kf.split(x):\n",
    "    xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                            learning_rate=0.02,\n",
    "                            n_estimators=8,\n",
    "                            max_depth=1,\n",
    "                            subsample=0.4,\n",
    "                            random_state=123).fit(x[train_index], y[train_index])\n",
    "    # make predictions for test data\n",
    "    predictions = xgb_model.predict(x[test_index])\n",
    "    actuals = y[test_index]\n",
    "    predictions = [round(value) for value in predictions]\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    print('Confusion Matrix :')\n",
    "    print(confusion_matrix(actuals, predictions))\n",
    "    print('Report : ')\n",
    "    print(classification_report(actuals, predictions))\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"Multiclass classification\")\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)\n",
    "for train_index, test_index in kf.split(x):\n",
    "    xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                            learning_rate=0.01,\n",
    "                            n_estimators=100,\n",
    "                            max_depth=3,\n",
    "                            subsample=0.6,\n",
    "                            random_state=123).fit(x[train_index], y[train_index])\n",
    "    predictions = xgb_model.predict(x[test_index])\n",
    "    actuals = y[test_index]\n",
    "    predictions = [round(value) for value in predictions]\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    print('Confusion Matrix :')\n",
    "    print(confusion_matrix(actuals, predictions))\n",
    "    print('Report : ')\n",
    "    print(classification_report(actuals, predictions))\n",
    "\n",
    "print(\"start of: regression\")\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)\n",
    "for train_index, test_index in kf.split(x):\n",
    "    xgb_model = xgb.XGBRegressor().fit(x[train_index], y[train_index])\n",
    "    predictions = xgb_model.predict(x[test_index])\n",
    "    actuals = y[test_index]\n",
    "    predictions = [round(value) for value in predictions]\n",
    "    # evaluate predictions\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    print('Confusion Matrix :')\n",
    "    print(confusion_matrix(actuals, predictions))\n",
    "    print('Report : ')\n",
    "    print(classification_report(actuals, predictions))\n",
    "    print(mean_squared_error(actuals, predictions))\n",
    "\n",
    "print(\"Parameter optimization\")\n",
    "\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "crf = GridSearchCV(xgb_model,\n",
    "                   {'max_depth': [2,4,6],'n_estimators': [5,3,8],'learning_rate': [0.1,0.01]}, verbose=1)\n",
    "crf.fit(x,y)\n",
    "print(crf.best_score_)\n",
    "print(crf.best_params_)\n",
    "\n",
    "print(\"End of regressor\")\n",
    "\n",
    "# The sklearn API models are picklable\n",
    "print(\"Pickling sklearn API models\")\n",
    "# must open in binary format to pickle\n",
    "pickle.dump(clf, open(\"Data/best_predict.pkl\", \"wb\"))\n",
    "clf2 = pickle.load(open(\"Data/best_predict.pkl\", \"rb\"))\n",
    "print(np.allclose(clf.predict(x), clf2.predict(x)))\n",
    "# Early-stopping\n",
    "\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05, random_state=54321)\n",
    "clf = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                            learning_rate=0.01,\n",
    "                            n_estimators=200,\n",
    "                            max_depth=6,\n",
    "                            subsample=0.8,\n",
    "                            random_state=54321)\n",
    "\n",
    "eval_set = [(x_test, y_test)]\n",
    "# fit model on training data\n",
    "clf.fit(x_train, y_train, early_stopping_rounds=20, eval_metric=\"auc\",eval_set=eval_set, verbose=True)\n",
    "# make predictions for test data\n",
    "y_pred = clf.predict(x_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Prediction and Output for Scoring\n",
    "\n",
    "Before we predict and export our score we need to apply the same changes to our test set as our training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_values=pd.read_csv('Data/df_test_enc_2.csv')\n",
    "test_values=test_values.drop(['Unnamed: 0'],axis=1)\n",
    "test_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_values = test_values.drop(['False_Col' 'True_Col'], axis=1)\n",
    "#test_values= test_values.fillna(test_values.mean())\n",
    "test_values=np.array(test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "std = RobustScaler()\n",
    "std.fit(test_values)\n",
    "test_values = std.transform(test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions using the testing set\n",
    "rf_pred = xgb_model.predict(test_values)\n",
    "rf_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_prediccion=pd.DataFrame(data=rf_pred,columns=['accepted'])\n",
    "L_prediccion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_prediccion.index.names=['row_id']\n",
    "L_prediccion['accepted']= L_prediccion['accepted'].astype(np.int64)\n",
    "L_prediccion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_prediccion.to_csv('Data/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_norm = preprocessing.scale(df_enc[int_cols], axis =0)\n",
    "df_trf = np.concatenate([data_norm,Features],axis=1)\n",
    "df_trf = pd.DataFrame(df_trf, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import models from scikit learn module:\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "#Generic function for making a classification model and accessing performance:\n",
    "\n",
    "def classification_model(model, data, predictors, outcome):\n",
    "    #Fit the model:\n",
    "    model.fit(data[predictors],data[outcome])\n",
    "  \n",
    "    #Make predictions on training set:\n",
    "    predictions = model.predict(data[predictors])\n",
    "  \n",
    "    #Print accuracy\n",
    "    accuracy = metrics.accuracy_score(predictions,data[outcome])\n",
    "    print (\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "    #Perform k-fold cross-validation with 5 folds\n",
    "    kf = KFold(data.shape[0], n_folds=5)\n",
    "    error = []\n",
    "    for train, test in kf:\n",
    "        # Filter training data\n",
    "        train_predictors = (data[predictors].iloc[train,:])\n",
    "    \n",
    "        # The target we're using to train the algorithm.\n",
    "        train_target = data[outcome].iloc[train]\n",
    "    \n",
    "        # Training the algorithm using the predictors and target.\n",
    "        model.fit(train_predictors, train_target)\n",
    "    \n",
    "        #Record error from each cross-validation run\n",
    "        error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\n",
    " \n",
    "    print (\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error)))\n",
    "\n",
    "    #Fit the model again so that it can be refered outside the function:\n",
    "    model.fit(data[predictors],data[outcome])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "predictors_Logistic= ['applicant_income_log','loan_amount_log','loan_purpose','co_applicant_False','co_applicant_True',\n",
    "           'property_type','tract_to_msa_md_income_pct_log','minority_population_pct_log','applicant_race',\n",
    "            'number_of_owner-occupied_units_log','applicant_sex','state_code_log',\n",
    "            'number_of_1_to_4_family_units_log','ffiecmedian_family_income_log']\n",
    "\n",
    "x_train = df[list(predictors_Logistic)].values\n",
    "y_train = df[\"accepted\"].values\n",
    "\n",
    "x_test=test_values[list(predictors_Logistic)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create logistic regression object\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#Predict Output\n",
    "predicted= model.predict(x_test)\n",
    "\n",
    "#Reverse encoding for predicted outcome\n",
    "#predicted = number.inverse_transform(predicted)\n",
    "\n",
    "#Store it to test dataset\n",
    "df['accepted']=predicted\n",
    "\n",
    "outcome_var = 'accepted'\n",
    "\n",
    "classification_model(model, df,predictors_Logistic,outcome_var)\n",
    "\n",
    "df.to_csv(\"Logistic_Prediction.csv\",columns=['row_ID','accepted'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
